---
description: null
globs: null
alwaysApply: false
type: auto
migrated: true
migration_date: '2025-06-07T14:32:13.676582'
---

# Agent Decision Confidence & Cross-Project Learning Rules

## 1. Decision Confidence Scoring

### 1.1. Objective
All autonomous decisions or proposals made by agents MUST be accompanied by a quantifiable confidence score. This score helps in routing, review prioritization, and risk assessment.

### 1.2. Requirements
- **Score Range**: Confidence scores MUST be on a normalized scale (e.g., 0.0 to 1.0, or 1 to 100).
- **Calculation Basis**: The agent MUST have a discernible (even if heuristic) basis for its confidence score. This could include:
    - Quality and completeness of input data.
    - Historical success rate of similar decisions.
    - Number of ambiguities or missing pieces of information encountered.
    - Complexity of the decision-making logic involved.
    - Agreement/disagreement from multiple internal models or reasoning paths.
    - For rule-based agents, how comprehensively the current context matches rule preconditions.
- **Metadata**: The confidence score SHOULD be accompanied by metadata explaining its basis (e.g., `{"source_data_completeness": 0.7, "historical_accuracy": 0.85, "ambiguity_penalty": -0.1}`)
- **Default State**: Agents SHOULD have a default or baseline confidence for routine operations. Significant deviations SHOULD be justified.

### 1.3. Usage in CoE & Frameworks
- **CoE Routing**: Proposals with confidence scores below a certain threshold MAY automatically be routed to a higher CoE review tier (see `coe_continuous_learning.mdc`).
- **Automated Approvals**: High-confidence, low-risk proposals MAY be eligible for automated approval (Tier 0 CoE) if they meet predefined criteria.
- **Learning Loops**: Confidence scores and their correlation with actual outcomes (post-CoE review or operational success) MUST be a key input for continuous learning loops.

### 1.4. Example (Agent Pseudocode)
```python
class MyDecisionAgent:
    def make_proposal(self, data):
        # ... decision logic ...
        confidence = self._calculate_confidence(data, result)
        proposal = {
            "action": result,
            "confidence_score": confidence,
            "confidence_metadata": self.last_confidence_basis
        }
        return proposal

    def _calculate_confidence(self, data, result):
        score = 1.0
        self.last_confidence_basis = {}
        if not data.get('critical_input'):
            score -= 0.3
            self.last_confidence_basis['missing_critical_input'] = True
        # ... other factors ...
        return max(0.0, min(1.0, score))
```

## 2. Cross-Project Learning Integration

### 2.1. Objective
To enable the system to learn and apply insights, best practices, and validated patterns from one project or agent context to others, fostering a globally improving VANTA ecosystem.

### 2.2. Mechanisms
- **Standardized Knowledge Format**: Learnings, rules, and patterns intended for cross-project sharing MUST be stored in a standardized, machine-readable format (e.g., structured MDC, YAML, or a dedicated KB schema).
- **Anonymization & Generalization**: Before sharing, project-specific sensitive data MUST be removed or anonymized. Learnings should be generalized where possible to increase applicability.
- **Centralized/Federated Knowledge Base**: A mechanism (e.g., `GlobalRuleRegistry`, `SharedBestPracticesKB`) for storing and discovering these cross-project learnings MUST exist. This could be centralized or a federated system querying project-level KBs.
- **`GlobalLearningAgent`**: A dedicated agent (or role within a `MetaAgent`) SHOULD be responsible for:
    - Identifying potentially shareable learnings from project-level CoE feedback and performance data.
    - Generalizing and anonymizing these learnings.
    - Submitting them to the global knowledge base (potentially via a Global CoE review).
    - Proactively suggesting relevant global learnings to individual projects/agents when appropriate contexts arise.
- **Subscription/Discovery**: Agents or project frameworks SHOULD be able to subscribe to or query the global knowledge base for relevant rules, patterns, or known issues.

### 2.3. Rule Integration
- New rules (MDC) created as a result of cross-project learning SHOULD be added to a global ruleset or a shared library that can be imported by projects.
- The `GlobalLearningAgent` can propose the creation or modification of global MDC rules based on observed cross-project patterns.

### 2.4. Governance
- The process for promoting a project-specific learning to a global standard MUST be governed, likely by a Global CoE or a similar high-level review body.

### 2.5. Example Flow
1. Project `Omega`'s CoE identifies a highly effective new MDC rule for validating API inputs, significantly reducing errors.
2. `Omega`'s `LearningSubsystem` flags this rule as a candidate for global sharing due to its generic applicability.
3. The `GlobalLearningAgent` picks up this candidate, anonymizes any project-specific details, and verifies its general utility.
4. The `GlobalLearningAgent` submits the generalized rule to the `GlobalRuleRegistry` (potentially after Global CoE approval).
5. Agent `Beta` in Project `Sigma`, when setting up a new API, queries the `GlobalRuleRegistry` and discovers this validated input validation rule, applying it to its project.

--- 
**Impact**: Enhances overall system intelligence, reduces redundant problem-solving across projects, and accelerates the adoption of best practices.
