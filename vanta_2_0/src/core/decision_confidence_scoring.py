#!/usr/bin/env python3
"""
Decision Confidence Scoring System - VANTA 2.0
Advanced system for measuring CoE decision quality and success probability
"""

import numpy as np
import json
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
import statistics
import logging
from collections import defaultdict

class ConfidenceLevel(Enum):
    VERY_LOW = "very_low"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"

class ExpertiseArea(Enum):
    SECURITY = "security"
    PERFORMANCE = "performance"
    INTEGRATION = "integration"
    COMPLIANCE = "compliance"
    AI_SYSTEMS = "ai_systems"
    ARCHITECTURE = "architecture"
    USER_EXPERIENCE = "user_experience"

@dataclass
class ExpertVote:
    expert_id: str
    expert_type: str
    recommendation: str  # "approve", "approve_with_modifications", "reject"
    confidence: float  # 0.0 to 1.0
    reasoning_quality: float  # 0.0 to 1.0 (assessed by reasoning analyzer)
    expertise_relevance: float  # 0.0 to 1.0 (how relevant expert's expertise is)
    vote_timestamp: datetime
    detailed_reasoning: str
    risk_assessment: Dict[str, Any]
    modification_suggestions: List[str]

@dataclass
class DecisionMetrics:
    consensus_strength: float
    reasoning_depth: float
    expertise_coverage: float
    risk_alignment: float
    historical_accuracy: float
    overall_confidence: float
    confidence_level: ConfidenceLevel
    predicted_success_probability: float
    recommended_oversight_level: str

@dataclass
class HistoricalDecision:
    decision_id: str
    decision_date: datetime
    confidence_score: float
    predicted_success: float
    actual_outcome: Optional[float]  # Set after implementation
    outcome_date: Optional[datetime]
    lessons_learned: List[str]

class ReasoningAnalyzer:
    """
    Analyzes the quality and depth of expert reasoning
    """
    
    def __init__(self):
        self.quality_indicators = {
            'evidence_based': ['data', 'metrics', 'benchmark', 'analysis', 'study'],
            'risk_awareness': ['risk', 'threat', 'vulnerability', 'mitigation', 'contingency'],
            'comprehensive': ['consider', 'alternative', 'impact', 'consequence', 'implication'],
            'specific': ['specific', 'detailed', 'precise', 'exact', 'particular'],
            'experience_based': ['experience', 'previous', 'similar', 'learned', 'observed']
        }
        
    def analyze_reasoning_quality(self, reasoning_text: str) -> Dict[str, float]:
        """
        Analyze the quality of expert reasoning based on multiple factors
        """
        reasoning_lower = reasoning_text.lower()
        word_count = len(reasoning_text.split())
        
        quality_scores = {}
        
        # Evidence-based reasoning
        evidence_score = self._calculate_indicator_score(reasoning_lower, 'evidence_based')
        quality_scores['evidence_based'] = evidence_score
        
        # Risk awareness
        risk_score = self._calculate_indicator_score(reasoning_lower, 'risk_awareness')
        quality_scores['risk_awareness'] = risk_score
        
        # Comprehensiveness
        comprehensive_score = self._calculate_indicator_score(reasoning_lower, 'comprehensive')
        quality_scores['comprehensive'] = comprehensive_score
        
        # Specificity
        specific_score = self._calculate_indicator_score(reasoning_lower, 'specific')
        quality_scores['specific'] = specific_score
        
        # Experience-based
        experience_score = self._calculate_indicator_score(reasoning_lower, 'experience_based')
        quality_scores['experience_based'] = experience_score
        
        # Length factor (longer reasoning generally indicates more thought)
        length_factor = min(1.0, word_count / 100)  # Normalize to 100 words
        quality_scores['depth'] = length_factor
        
        # Overall quality score
        overall_quality = np.mean(list(quality_scores.values()))
        quality_scores['overall'] = overall_quality
        
        return quality_scores
    
    def _calculate_indicator_score(self, text: str, indicator_type: str) -> float:
        """Calculate score for specific quality indicator"""
        indicators = self.quality_indicators[indicator_type]
        matches = sum(1 for indicator in indicators if indicator in text)
        return min(1.0, matches / len(indicators))

class ConsensusAnalyzer:
    """
    Analyzes consensus strength and agreement patterns among experts
    """
    
    def calculate_consensus_strength(self, expert_votes: List[ExpertVote]) -> Dict[str, float]:
        """
        Calculate various measures of consensus strength
        """
        if not expert_votes:
            return {"overall": 0.0, "recommendation_agreement": 0.0, "confidence_variance": 1.0}
        
        # Recommendation agreement
        recommendations = [vote.recommendation for vote in expert_votes]
        most_common = max(set(recommendations), key=recommendations.count)
        agreement_ratio = recommendations.count(most_common) / len(recommendations)
        
        # Confidence variance (lower variance = higher consensus)
        confidences = [vote.confidence for vote in expert_votes]
        confidence_variance = statistics.variance(confidences) if len(confidences) > 1 else 0.0
        confidence_consistency = 1.0 - min(1.0, confidence_variance)
        
        # Weighted consensus (considering expert relevance)
        weighted_agreement = self._calculate_weighted_consensus(expert_votes)
        
        # Expert expertise alignment
        expertise_alignment = self._calculate_expertise_alignment(expert_votes)
        
        # Overall consensus strength
        overall_consensus = np.mean([
            agreement_ratio, 
            confidence_consistency, 
            weighted_agreement, 
            expertise_alignment
        ])
        
        return {
            "overall": overall_consensus,
            "recommendation_agreement": agreement_ratio,
            "confidence_consistency": confidence_consistency,
            "weighted_agreement": weighted_agreement,
            "expertise_alignment": expertise_alignment,
            "confidence_variance": confidence_variance
        }
    
    def _calculate_weighted_consensus(self, expert_votes: List[ExpertVote]) -> float:
        """Calculate consensus weighted by expert relevance and confidence"""
        if not expert_votes:
            return 0.0
        
        # Weight votes by expertise relevance and confidence
        weighted_votes = defaultdict(float)
        total_weight = 0.0
        
        for vote in expert_votes:
            weight = vote.expertise_relevance * vote.confidence
            weighted_votes[vote.recommendation] += weight
            total_weight += weight
        
        if total_weight == 0:
            return 0.0
        
        # Find the strongest weighted recommendation
        max_weight = max(weighted_votes.values())
        return max_weight / total_weight
    
    def _calculate_expertise_alignment(self, expert_votes: List[ExpertVote]) -> float:
        """Calculate how well expert expertise aligns with the decision"""
        if not expert_votes:
            return 0.0
        
        relevance_scores = [vote.expertise_relevance for vote in expert_votes]
        return np.mean(relevance_scores)

class HistoricalAccuracyTracker:
    """
    Tracks historical decision accuracy to improve future predictions
    """
    
    def __init__(self):
        self.historical_decisions: List[HistoricalDecision] = []
        self.accuracy_metrics = {}
        
    def add_historical_decision(self, decision: HistoricalDecision):
        """Add a historical decision for tracking"""
        self.historical_decisions.append(decision)
        self._update_accuracy_metrics()
    
    def calculate_predictor_accuracy(self) -> Dict[str, float]:
        """Calculate accuracy of confidence score predictions"""
        completed_decisions = [
            d for d in self.historical_decisions 
            if d.actual_outcome is not None
        ]
        
        if not completed_decisions:
            return {"overall_accuracy": 0.5, "prediction_correlation": 0.0}
        
        # Calculate correlation between predicted and actual success
        predicted = [d.predicted_success for d in completed_decisions]
        actual = [d.actual_outcome for d in completed_decisions]
        
        correlation = np.corrcoef(predicted, actual)[0, 1] if len(predicted) > 1 else 0.0
        
        # Calculate accuracy based on threshold
        correct_predictions = sum(
            1 for p, a in zip(predicted, actual)
            if (p >= 0.7 and a >= 0.7) or (p < 0.7 and a < 0.7)
        )
        accuracy = correct_predictions / len(completed_decisions)
        
        return {
            "overall_accuracy": accuracy,
            "prediction_correlation": max(0.0, correlation),  # Ensure non-negative
            "sample_size": len(completed_decisions)
        }
    
    def get_similar_decision_outcomes(self, current_decision_context: Dict[str, Any]) -> List[float]:
        """Find outcomes of similar historical decisions"""
        similar_outcomes = []
        
        for decision in self.historical_decisions:
            if decision.actual_outcome is not None:
                # Simple similarity check (could be enhanced with ML)
                similar_outcomes.append(decision.actual_outcome)
        
        return similar_outcomes[-10:]  # Return last 10 similar decisions
    
    def _update_accuracy_metrics(self):
        """Update internal accuracy metrics"""
        self.accuracy_metrics = self.calculate_predictor_accuracy()

class DecisionConfidenceScorer:
    """
    Main class for calculating comprehensive decision confidence scores
    """
    
    def __init__(self):
        self.reasoning_analyzer = ReasoningAnalyzer()
        self.consensus_analyzer = ConsensusAnalyzer()
        self.historical_tracker = HistoricalAccuracyTracker()
        
        # Load historical data if available
        self._load_historical_data()
    
    def calculate_decision_confidence(self, 
                                    expert_votes: List[ExpertVote],
                                    decision_context: Dict[str, Any]) -> DecisionMetrics:
        """
        Calculate comprehensive confidence metrics for a CoE decision
        """
        # Analyze consensus strength
        consensus_metrics = self.consensus_analyzer.calculate_consensus_strength(expert_votes)
        
        # Analyze reasoning quality
        reasoning_scores = []
        for vote in expert_votes:
            quality = self.reasoning_analyzer.analyze_reasoning_quality(vote.detailed_reasoning)
            reasoning_scores.append(quality['overall'])
        
        reasoning_depth = np.mean(reasoning_scores) if reasoning_scores else 0.0
        
        # Calculate expertise coverage
        expertise_coverage = self._calculate_expertise_coverage(expert_votes, decision_context)
        
        # Assess risk alignment among experts
        risk_alignment = self._assess_risk_alignment(expert_votes)
        
        # Get historical accuracy factor
        historical_accuracy = self._get_historical_accuracy_factor()
        
        # Calculate overall confidence
        overall_confidence = self._calculate_overall_confidence(
            consensus_metrics['overall'],
            reasoning_depth,
            expertise_coverage,
            risk_alignment,
            historical_accuracy
        )
        
        # Determine confidence level
        confidence_level = self._determine_confidence_level(overall_confidence)
        
        # Predict implementation success probability
        success_probability = self._predict_success_probability(
            overall_confidence, consensus_metrics, decision_context
        )
        
        # Recommend oversight level
        oversight_level = self._recommend_oversight_level(overall_confidence, success_probability)
        
        return DecisionMetrics(
            consensus_strength=consensus_metrics['overall'],
            reasoning_depth=reasoning_depth,
            expertise_coverage=expertise_coverage,
            risk_alignment=risk_alignment,
            historical_accuracy=historical_accuracy,
            overall_confidence=overall_confidence,
            confidence_level=confidence_level,
            predicted_success_probability=success_probability,
            recommended_oversight_level=oversight_level
        )
    
    def _calculate_expertise_coverage(self, 
                                    expert_votes: List[ExpertVote], 
                                    decision_context: Dict[str, Any]) -> float:
        """Calculate how well expert expertise covers the decision requirements"""
        required_expertise = decision_context.get('required_expertise', [])
        if not required_expertise:
            return 1.0  # If no specific requirements, assume full coverage
        
        expert_types = set(vote.expert_type for vote in expert_votes)
        coverage = len(expert_types.intersection(set(required_expertise))) / len(required_expertise)
        return min(1.0, coverage)
    
    def _assess_risk_alignment(self, expert_votes: List[ExpertVote]) -> float:
        """Assess how well experts align on risk assessment"""
        if not expert_votes:
            return 0.0
        
        # Extract risk scores from each expert
        risk_scores = []
        for vote in expert_votes:
            risk_data = vote.risk_assessment
            if 'overall_risk' in risk_data:
                # Convert risk level to numeric score
                risk_map = {'low': 0.2, 'medium': 0.5, 'high': 0.8, 'critical': 1.0}
                risk_scores.append(risk_map.get(risk_data['overall_risk'].lower(), 0.5))
        
        if not risk_scores:
            return 0.5
        
        # Calculate variance in risk assessment
        risk_variance = statistics.variance(risk_scores) if len(risk_scores) > 1 else 0.0
        risk_alignment = 1.0 - min(1.0, risk_variance * 4)  # Scale variance to 0-1
        
        return max(0.0, risk_alignment)
    
    def _get_historical_accuracy_factor(self) -> float:
        """Get historical accuracy factor for confidence adjustment"""
        accuracy_metrics = self.historical_tracker.calculate_predictor_accuracy()
        return accuracy_metrics.get('overall_accuracy', 0.7)  # Default to 70% if no history
    
    def _calculate_overall_confidence(self, 
                                    consensus: float,
                                    reasoning: float,
                                    expertise: float,
                                    risk_alignment: float,
                                    historical: float) -> float:
        """Calculate weighted overall confidence score"""
        weights = {
            'consensus': 0.25,
            'reasoning': 0.20,
            'expertise': 0.20,
            'risk_alignment': 0.15,
            'historical': 0.20
        }
        
        weighted_score = (
            weights['consensus'] * consensus +
            weights['reasoning'] * reasoning +
            weights['expertise'] * expertise +
            weights['risk_alignment'] * risk_alignment +
            weights['historical'] * historical
        )
        
        return min(1.0, max(0.0, weighted_score))
    
    def _determine_confidence_level(self, overall_confidence: float) -> ConfidenceLevel:
        """Determine categorical confidence level"""
        if overall_confidence >= 0.9:
            return ConfidenceLevel.VERY_HIGH
        elif overall_confidence >= 0.75:
            return ConfidenceLevel.HIGH
        elif overall_confidence >= 0.6:
            return ConfidenceLevel.MEDIUM
        elif overall_confidence >= 0.4:
            return ConfidenceLevel.LOW
        else:
            return ConfidenceLevel.VERY_LOW
    
    def _predict_success_probability(self, 
                                   confidence: float,
                                   consensus_metrics: Dict[str, float],
                                   decision_context: Dict[str, Any]) -> float:
        """Predict implementation success probability"""
        # Base prediction on confidence score
        base_probability = confidence
        
        # Adjust based on consensus strength
        consensus_adjustment = (consensus_metrics['overall'] - 0.5) * 0.2
        
        # Adjust based on project complexity
        complexity = decision_context.get('complexity_score', 0.5)
        complexity_adjustment = (0.5 - complexity) * 0.1
        
        # Adjust based on similar historical decisions
        similar_outcomes = self.historical_tracker.get_similar_decision_outcomes(decision_context)
        if similar_outcomes:
            historical_avg = np.mean(similar_outcomes)
            historical_adjustment = (historical_avg - 0.5) * 0.1
        else:
            historical_adjustment = 0.0
        
        predicted_probability = base_probability + consensus_adjustment + complexity_adjustment + historical_adjustment
        
        return min(1.0, max(0.0, predicted_probability))
    
    def _recommend_oversight_level(self, confidence: float, success_probability: float) -> str:
        """Recommend appropriate oversight level based on confidence"""
        avg_score = (confidence + success_probability) / 2
        
        if avg_score >= 0.85:
            return "minimal"  # Light monitoring
        elif avg_score >= 0.7:
            return "standard"  # Regular check-ins
        elif avg_score >= 0.55:
            return "enhanced"  # Weekly reviews
        else:
            return "intensive"  # Daily monitoring
    
    def update_decision_outcome(self, 
                              decision_id: str, 
                              actual_outcome: float,
                              lessons_learned: List[str]):
        """Update historical data with actual decision outcome"""
        historical_decision = HistoricalDecision(
            decision_id=decision_id,
            decision_date=datetime.now(),
            confidence_score=0.0,  # Would be set from original decision
            predicted_success=0.0,  # Would be set from original decision
            actual_outcome=actual_outcome,
            outcome_date=datetime.now(),
            lessons_learned=lessons_learned
        )
        
        self.historical_tracker.add_historical_decision(historical_decision)
        self._save_historical_data()
    
    def _load_historical_data(self):
        """Load historical decision data"""
        # Implementation would load from persistent storage
        pass
    
    def _save_historical_data(self):
        """Save historical decision data"""
        # Implementation would save to persistent storage
        pass

# Enhanced reporting and visualization
class ConfidenceReporter:
    """
    Generate detailed reports and visualizations for confidence scores
    """
    
    def __init__(self, scorer: DecisionConfidenceScorer):
        self.scorer = scorer
    
    def generate_confidence_report(self, 
                                 decision_metrics: DecisionMetrics,
                                 expert_votes: List[ExpertVote]) -> Dict[str, Any]:
        """Generate comprehensive confidence report"""
        return {
            "executive_summary": {
                "overall_confidence": f"{decision_metrics.overall_confidence:.1%}",
                "confidence_level": decision_metrics.confidence_level.value,
                "success_probability": f"{decision_metrics.predicted_success_probability:.1%}",
                "recommended_action": self._recommend_action(decision_metrics),
                "oversight_level": decision_metrics.recommended_oversight_level
            },
            "detailed_analysis": {
                "consensus_strength": {
                    "score": f"{decision_metrics.consensus_strength:.1%}",
                    "interpretation": self._interpret_consensus(decision_metrics.consensus_strength)
                },
                "reasoning_quality": {
                    "score": f"{decision_metrics.reasoning_depth:.1%}",
                    "interpretation": self._interpret_reasoning(decision_metrics.reasoning_depth)
                },
                "expertise_coverage": {
                    "score": f"{decision_metrics.expertise_coverage:.1%}",
                    "interpretation": self._interpret_expertise(decision_metrics.expertise_coverage)
                },
                "risk_alignment": {
                    "score": f"{decision_metrics.risk_alignment:.1%}",
                    "interpretation": self._interpret_risk_alignment(decision_metrics.risk_alignment)
                }
            },
            "expert_analysis": [
                {
                    "expert": vote.expert_id,
                    "recommendation": vote.recommendation,
                    "confidence": f"{vote.confidence:.1%}",
                    "reasoning_quality": f"{vote.reasoning_quality:.1%}",
                    "expertise_relevance": f"{vote.expertise_relevance:.1%}"
                }
                for vote in expert_votes
            ],
            "recommendations": {
                "proceed": decision_metrics.overall_confidence >= 0.6,
                "conditions": self._generate_conditions(decision_metrics),
                "monitoring_requirements": self._generate_monitoring_requirements(decision_metrics),
                "risk_mitigation": self._generate_risk_mitigation(decision_metrics)
            }
        }
    
    def _recommend_action(self, metrics: DecisionMetrics) -> str:
        """Recommend action based on confidence metrics"""
        if metrics.overall_confidence >= 0.8:
            return "Proceed with implementation"
        elif metrics.overall_confidence >= 0.6:
            return "Proceed with enhanced monitoring"
        elif metrics.overall_confidence >= 0.4:
            return "Address concerns before proceeding"
        else:
            return "Significant revision required"
    
    def _interpret_consensus(self, score: float) -> str:
        """Interpret consensus strength score"""
        if score >= 0.8:
            return "Strong expert agreement"
        elif score >= 0.6:
            return "Moderate expert agreement"
        else:
            return "Low expert agreement - consider additional review"
    
    def _interpret_reasoning(self, score: float) -> str:
        """Interpret reasoning quality score"""
        if score >= 0.8:
            return "High-quality, evidence-based reasoning"
        elif score >= 0.6:
            return "Adequate reasoning with room for improvement"
        else:
            return "Reasoning quality concerns - request more detail"
    
    def _interpret_expertise(self, score: float) -> str:
        """Interpret expertise coverage score"""
        if score >= 0.8:
            return "Comprehensive expertise coverage"
        elif score >= 0.6:
            return "Good expertise coverage with minor gaps"
        else:
            return "Significant expertise gaps - consider additional experts"
    
    def _interpret_risk_alignment(self, score: float) -> str:
        """Interpret risk alignment score"""
        if score >= 0.8:
            return "Strong alignment on risk assessment"
        elif score >= 0.6:
            return "Moderate risk assessment alignment"
        else:
            return "Divergent risk views - requires discussion"
    
    def _generate_conditions(self, metrics: DecisionMetrics) -> List[str]:
        """Generate conditions for proceeding"""
        conditions = []
        
        if metrics.consensus_strength < 0.7:
            conditions.append("Achieve stronger expert consensus")
        
        if metrics.reasoning_depth < 0.6:
            conditions.append("Provide more detailed reasoning and evidence")
        
        if metrics.expertise_coverage < 0.8:
            conditions.append("Include additional expert perspectives")
        
        if metrics.predicted_success_probability < 0.7:
            conditions.append("Address factors reducing success probability")
        
        return conditions if conditions else ["No additional conditions required"]
    
    def _generate_monitoring_requirements(self, metrics: DecisionMetrics) -> List[str]:
        """Generate monitoring requirements"""
        requirements = ["Regular progress tracking"]
        
        if metrics.overall_confidence < 0.8:
            requirements.append("Enhanced milestone reviews")
        
        if metrics.consensus_strength < 0.7:
            requirements.append("Ongoing expert consultation")
        
        if metrics.predicted_success_probability < 0.8:
            requirements.append("Early warning system for issues")
        
        return requirements
    
    def _generate_risk_mitigation(self, metrics: DecisionMetrics) -> List[str]:
        """Generate risk mitigation recommendations"""
        mitigations = []
        
        if metrics.overall_confidence < 0.7:
            mitigations.append("Develop comprehensive contingency plans")
        
        if metrics.consensus_strength < 0.6:
            mitigations.append("Establish expert review board for ongoing decisions")
        
        if metrics.predicted_success_probability < 0.7:
            mitigations.append("Implement phased rollout with go/no-go gates")
        
        return mitigations if mitigations else ["Standard risk monitoring procedures"]

# Usage example and testing
if __name__ == "__main__":
    # Initialize the confidence scoring system
    scorer = DecisionConfidenceScorer()
    reporter = ConfidenceReporter(scorer)
    
    # Create sample expert votes
    sample_votes = [
        ExpertVote(
            expert_id="security_expert_001",
            expert_type="security",
            recommendation="approve_with_modifications",
            confidence=0.85,
            reasoning_quality=0.8,
            expertise_relevance=0.9,
            vote_timestamp=datetime.now(),
            detailed_reasoning="The security framework is well-designed with proper authentication and encryption. However, I recommend additional AI security hardening including prompt injection protection and enhanced rate limiting. The audit logging approach is comprehensive and meets compliance requirements.",
            risk_assessment={"overall_risk": "medium", "security_risk": "low"},
            modification_suggestions=["AI security hardening", "Enhanced rate limiting"]
        ),
        ExpertVote(
            expert_id="performance_expert_001", 
            expert_type="performance",
            recommendation="approve_with_modifications",
            confidence=0.78,
            reasoning_quality=0.75,
            expertise_relevance=0.85,
            vote_timestamp=datetime.now(),
            detailed_reasoning="The performance targets are ambitious but achievable with proper caching implementation. The 5-second AI analysis target requires optimization but is realistic. I recommend multi-level caching and async processing for complex operations.",
            risk_assessment={"overall_risk": "medium", "performance_risk": "medium"},
            modification_suggestions=["Multi-level caching", "Async processing optimization"]
        ),
        ExpertVote(
            expert_id="integration_expert_001",
            expert_type="integration",
            recommendation="approve",
            confidence=0.82,
            reasoning_quality=0.85,
            expertise_relevance=0.8,
            vote_timestamp=datetime.now(),
            detailed_reasoning="Integration approach is sound with good backward compatibility planning. Migration strategy is well-thought-out and rollback procedures are adequate. The phased implementation reduces integration risks significantly.",
            risk_assessment={"overall_risk": "low", "integration_risk": "low"},
            modification_suggestions=[]
        )
    ]
    
    # Sample decision context
    decision_context = {
        "complexity_score": 0.75,
        "required_expertise": ["security", "performance", "integration"],
        "budget": 139000,
        "timeline_weeks": 14,
        "strategic_importance": "high"
    }
    
    # Calculate confidence metrics
    metrics = scorer.calculate_decision_confidence(sample_votes, decision_context)
    
    # Generate comprehensive report
    report = reporter.generate_confidence_report(metrics, sample_votes)
    
    print("ðŸŽ¯ Decision Confidence Scoring Complete!")
    print("=" * 50)
    print(f"Overall Confidence: {metrics.overall_confidence:.1%}")
    print(f"Confidence Level: {metrics.confidence_level.value}")
    print(f"Success Probability: {metrics.predicted_success_probability:.1%}")
    print(f"Recommended Oversight: {metrics.recommended_oversight_level}")
    print(f"Consensus Strength: {metrics.consensus_strength:.1%}")
    print(f"Reasoning Quality: {metrics.reasoning_depth:.1%}")
    print("\nâœ… Enhanced Decision Confidence Scoring System initialized!")
    print("ðŸ“Š Advanced analytics, prediction accuracy tracking, and confidence reporting ready") 